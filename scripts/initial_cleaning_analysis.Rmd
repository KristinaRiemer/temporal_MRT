---
title: 'Temporal MRT Project: analysis pipeline with two datasets'
output:
  html_notebook: default
  pdf_document: default
---

### R packages

```{r echo=TRUE}
library(rdataretriever)
library(dplyr)
library(ggplot2)
library(tidyr)
library(testthat)
library(ncdf4)
library(rgdal)
```

### Temperature data

#### TODO

* Create test for reasonable temp range for Florida location
* Figure out baseline temperature anomaly value
* Determine why, for coordinates and time, a list of length 359 is returned instead of a single value

Requirements: 

* Global in extent
* Year range of 1950-now
* Fairly fine spatial resolution

**University of Delaware** dataset includes all of these: 

* [Website](https://www.esrl.noaa.gov/psd/data/gridded/data.UDel_AirT_Precip.html)
* [Another website](https://climatedataguide.ucar.edu/climate-data/global-land-precipitation-and-temperature-willmott-matsuura-university-delaware)
* Air Temperature Monthly Mean V4.01 Surface
* Temporal range is monthly values for 1900-2014
* 0.5 degree by 0.5 degree latitude-longitude grid

For what this grid size means, see the blue grid around Portal
(red grid if 1 degree by 1 degree)

![](../portal_map_grid.png)

```{r}
if(!file.exists("../data/temperature/air.mon.mean.v401.nc")){
  download.file("ftp://ftp.cdc.noaa.gov/Datasets/udel.airt.precip/air.mon.mean.v401.nc", "../data/temperature/air.mon.mean.v401.nc")
}
```
Download University of Delaware temperature data if it isn't already. 
```{r}
temp = nc_open("../data/temperature/air.mon.mean.v401.nc")
```
Read in temperature data. 
```{r}
temp
```
```{r}
temp_variable_all = ncvar_get(temp, attributes(temp$var)$names)
```

#### Metadata notes

* See [Expert Guidance page](https://climatedataguide.ucar.edu/climate-data/global-land-precipitation-and-temperature-willmott-matsuura-university-delaware) 
on UCAR Climate Data Guide
* See [metadata page](http://climate.geog.udel.edu/~climate/html_pages/Global2011/README.GlobalTsT2011.html)
from NOAA page
* Mostly compiled from different NOAA Global Historical Climatology Network (GHCN)
* Point estimates instead of grid cell averages? 
* Interpolation for grid node (i.e., center) for each 0.5 degree by 0.5 degree grid
* Missing values indicated with large -999... number (-9.96920996838687e+36)
    * R automatically turned no values into NAs? 
* `r temp$nvars` variable = `r attributes(temp$var)$names`
    * Missing data values is `r ncatt_get(temp, attributes(temp$var)$names)$missing_value`
    * Actual range in temp values is `r ncatt_get(temp, attributes(temp$var)$names)$actual_range`
    * Valid range in temp values is `r ncatt_get(temp, attributes(temp$var)$names)$valid_range`
* `r temp$ndims` dimensions = `r attributes(temp$dim)$names`
    * Max dimension limits are: 
        1. `r attributes(temp$dim)$names[1]` (limit: `r dim(temp_variable_all)[1]`)
        2. `r attributes(temp$dim)$names[2]` (limit: `r dim(temp_variable_all)[2]`)
        3. `r attributes(temp$dim)$names[3]` (limit: `r dim(temp_variable_all)[3]`)
* Lat/lon dimension details
    * [lon, lat]
    * Coordinates are center point of grid cell
        * e.g., far upper left grid is 0.25, 89.75
        * e.g., far lower right grid is 359.75, 87.75
    * Hemisphere orientation from map of Dec 2014 temps on NOAA page?
![](../map_dec14_website.png)
        * Positive latitudes are northern hemisphere, negative are southern
        * Longitudes from 0-180 are eastern hemisphere, from 180-360 are western hemisphere? 
        * Latitude is inverted? 
            * Lat indices 0-180 are northern hemisphere, 180-360 are southern hemisphere
            * Lon indices 0-360 are eastern hemisphere, 360-720 are western hemisphere
        * Test this by picking a point with the same coordinates in eastern and western hemisphere but one is in middle of ocean
* Time dimension details
    * Year range: 1900 - 2014
    * 115 years x 12 months = 1380 time values total
    * Where did actual range of 0 - 1007328 come from? 
    * Selection of time values: 
        * Jan 1900 = 1; Feb 1900 = 2; Dec 1900 = 11
        * Jan 1901 = 12
        * See function for converting from year + month in Python script
* Extracting values
    * Use temp_variable_all[lon, lat, time]
    * Coordinate uses integer index values, not grid mid-point values? 
    * Example is Denver in January 1901 (lon = , lat = 39.75, time = 12)

```{r}
dim(temp_variable_all)
singleyear_allcoords = temp_variable_all[0:720, 0:360, 12]
```

```{r}
if(!file.exists("../data/temperature/single_year_temp.csv")){
  single_year_temp = expand.grid(1:359, 1:719)
  names(single_year_temp) = c("lat", "lon")
  single_year_temp$temp = NA
  for(i in 1:nrow(single_year_temp)){
    single_year_temp[i, "temp"] = temp_variable_all[single_year_temp$lon[i], single_year_temp$lat[i], 1380]
  }
  write.csv(single_year_temp, "../data/temperature/single_year_temp.csv")
}
```
Get and save as CSV a single year of the temperature dataset for plotting purposes. 

```{r}
single_year_temp = read.csv("../data/temperature/single_year_temp.csv")
single_year_temp
```
Read in single year of temperature data. 

### Dataset 1: Portal

#### TODO

* Retain only adults; age column == Z? 
* Retain each individual only once? 
* Choose which experimental treatments to use

```{r}
if(!file.exists("../data/raw_portal/portal_dev_rodent_trapping.csv")){
  rdataretriever::install(dataset = "portal-dev", connection = "csv", data_dir = "../data/raw_portal/")
}
```
Download all Portal data using dataretriever if it isn't already. 
```{r}
portal_occurrences = read.csv("../data/raw_portal/portal_dev_rodent.csv")
dim(portal_occurrences)
head(portal_occurrences)
```
Read in Portal mammal occurrence data. 

```{r}
#colnames(portal_occurrences)
unique(portal_occurrences$age)
head(unique(portal_occurrences$tag), n = 20)
head(unique(portal_occurrences$ltag), n = 20)
```
Start looking at age class and tags for each occurrence. 
```{r}
portal_species = read.csv("../data/raw_portal/portal_dev_rodent_species.csv")
portal_species
```
Read in species data to remove non-rodent species from occurrence data. 
```{r}
table(portal_species$rodent)
rodent_list = portal_species %>% 
  filter(rodent == 1)
rodent_list
```
Generate list of rodent species. 
```{r}
by_sp_yr = portal_occurrences %>% 
  filter(!is.na(wgt), !is.na(species), species %in% rodent_list$species_code) %>% 
  group_by(species, yr) %>% 
  summarize(mass_mean = mean(wgt), 
            mass_sd = sd(wgt))
by_sp_yr
```
Create new dataframe containing mean and standard deviation of each species' mass for each year, 
removing rows with no weight or no species ID and retaining only rodents. 

The minimum mean mass is `r min(by_sp_yr$mass_mean)` and max is `r max(by_sp_yr$mass_mean)`. 
There are `r length(unique(by_sp_yr$species))` species across `r length(unique(by_sp_yr$yr))` years. 
```{r}
ggplot(data = by_sp_yr, aes(x = yr, y = mass_mean)) +
  geom_errorbar(aes(ymin = mass_mean - mass_sd, ymax = mass_mean + mass_sd)) +
  geom_point() +
  facet_wrap(~species, scale = "free_y")
```
Plot mass mean and sd by species across years.

### Dataset 2: Fray Jorge

##### Notes

* [Mammal metadata](http://www.esapubs.org/archive/ecol/E094/084/Fray_Jorge_Small_Mammal_Metadata.php) 
from ESA Ecological Archives

#### TODO

* Choose which experimental treatments to use
* Get corresponding species names for species code column (sp) from Table 2 in
[metadata](http://www.esapubs.org/archive/ecol/E094/084/Fray_Jorge_Small_Mammal_Metadata.php)
* Double check that mass column (wt) is in grams
* Data only goes to 2005, but the project has been funded since then; are there more years? 
* Determine if precip data also contains temperature
* Choose which experimental treatments to use (trt col)
* Retain only adults (sst column?) 
* Retain each individual only once (AnimalID col)? 

```{r}
if(!file.exists("../data/raw_frayjorge/fray_jorge_ecology_mammals.csv")){
  rdataretriever::install(dataset = "fray-jorge-ecology", connection = "csv", data_dir = "../data/raw_frayjorge/")
}
```
Download all Fray Jorge data using dataretriever if it isn't already. 

Note: they use periods for missing data
```{r}
fray_occurrences = read.csv("../data/raw_frayjorge/fray_jorge_ecology_mammals.csv", na.strings = ".")
dim(fray_occurrences)
head(fray_occurrences)
```
Read in Fray Jorge mammal occurrence data. 
```{r}
fray_occurrences = fray_occurrences %>% 
  separate(mo, into = c("year", "month"), sep = 4, remove = FALSE, convert = TRUE)
```
Separate mo column into year and month columns. 
```{r}
by_sp_yr_fray = fray_occurrences %>% 
  filter(!is.na(wt), !is.na(sp)) %>% 
  group_by(sp, year) %>% 
  summarize(mass_mean = mean(wt), 
            mass_sd = sd(wt))
by_sp_yr_fray
```
Create new dataframe containing mean and standard deviation of each species' mass for each year, 
removing rows with no weight or no species ID. 

The minimum mean mass is `r min(by_sp_yr_fray$mass_mean)` and max is `r max(by_sp_yr_fray$mass_mean)`. 
There are `r length(unique(by_sp_yr_fray$sp))` species across `r length(unique(by_sp_yr_fray$year))` years. 
```{r}
ggplot(data = by_sp_yr_fray, aes(x = year, y = mass_mean)) +
  geom_point() +
  geom_errorbar(aes(ymin = mass_mean - mass_sd, ymax = mass_mean + mass_sd)) +
  facet_wrap(~sp, scales = "free_y")
```
Plot mass mean and sd by species across years.
```{r}
ggplot(fray_occurrences, aes(x = year)) +
  geom_bar() +
  facet_wrap(~sp, scales = "free_y")
```
Plot number of individuals of each species across years. 

### Dataset 3: Thibault et al., 2011

Ecological Archives documentation of data [here](http://esapubs.org/archive/ecol/E092/201/). 

#### TODO

* Determine if there is even enough data (number of individuals, year range) here to use


```{r}
if(!file.exists("../data/raw_thibault/mammal_community_db_communities.csv")){
  rdataretriever::install(dataset = "mammal-community-db", connection = "csv", data_dir = "../data/raw_thibault/")
}
```
Download all data from Thibault data paper using dataretriever if it isn't already. 

```{r}
thibault_df = read.csv("../data/raw_thibault/mammal_community_db_communities.csv")
head(thibault_df)
```

```{r}
dim(thibault_df)
length(which(!is.na(thibault_df$mass)))
```

```{r}
thibault_df_mass = thibault_df %>% 
  filter(!is.na(mass))
thibault_df_mass
```
Of `r length(unique(thibault_df$site_id))` sites in the entire dataset,
`r length(unique(thibault_df_mass$site_id))` have some mass measures. For sites
with masses, do all occurrences have mass values? 

### Site locations

#### TODO

* Get latitude and longitude for each dataset's site
* Document source for each of these

```{r}
site_locations = data.frame()
```
Create empty dataframe that will store location values. 

#### Portal

Notes

* From [Ernest et al., 2016](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/15-2115.1)
* Couldn't find location information in retriever versions of Portal data
* Expect Portal to be about -109.063, 31.9107
* In UTM zone 12

#### TODO 

* Add test for UTM to lat/lon conversion?

```{r}
if(!file.exists("../data/raw_portal/Portal_UTMCoords.csv")){
  download.file("https://esajournals.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1890%2F15-2115.1&file=ecy1360-sup-0001-DataS1.zip", 
               "../data/raw_portal/ecy1360-sup-0001-datas1.zip")
  unzip("../data/raw_portal/ecy1360-sup-0001-datas1.zip", exdir = "../data/raw_portal/")
}
```
Download data containing Portal coordinates. 
```{r}
portal_coords = read.csv("../data/raw_portal/Portal_UTMCoords.csv")
```
Read in particular dataset with coordinates. 
```{r}
portal_mean_UTM = SpatialPoints(cbind(mean(portal_coords$east), mean(portal_coords$north)), proj4string = CRS("+proj=utm +zone=12 +datum=WGS84"))
portal_mean_latlon = as.data.frame(spTransform(portal_mean_UTM, CRS("+proj=longlat +datum=WGS84")))
```
Generate variables for each coordinate direction, then convert from UTM to lat/lon. 
```{r}
site_locations = site_locations %>% 
  bind_rows(c(site_name = "portal", 
              lon = portal_mean_latlon$coords.x1, 
              lat = portal_mean_latlon$coords.x2))
site_locations
```
Add Portal lat/lon to locations dataframe. 

#### Fray Jorge

Notes

* From "Materials and Methods" section in Aguilera et al. (2015)
![](../fray_jorge_location.png)
* Expect latitude to be -30ish and longitude to be -71ish

#### TODO

* Find second source for Fray Jorge coordinates
* Figure out why coordinates are in ocean on temp grid

```{r}
deg_min_sec_to_dec = function(degree, minute, second, direction = NULL){
  decimal_degree = degree + (minute * (1/60)) + (second * (1/60) * (1/60))
  if(direction == "S" | direction == "W"){
    decimal_degree = -decimal_degree
  }
  return(decimal_degree)
}
```
Function to convert degree minute second coordinates to decimal degrees. 
```{r}
fray_jorge_lat = deg_min_sec_to_dec(30, 38, 0, "S")
fray_jorge_lon = deg_min_sec_to_dec(71, 40, 0, "W")
```
Use function to convert Fray Jorge coordinates. 
```{r}
site_locations = site_locations %>% 
  bind_rows(c(site_name = "fray_jorge", 
              lon = fray_jorge_lon, 
              lat = fray_jorge_lat))
site_locations
```
Add converted Fray Jorge coordinates to locations dataframe. 
```{r}
site_locations$lon = as.numeric(site_locations$lon)
site_locations$lat = as.numeric(site_locations$lat)
```
Change coordinates columns to numeric type. 

### Site temperatures

#### TODO

* Test Portal temps against the on-site temperature measurements
* Delete manual change in Fray Jorge longitude

#### Notes

* Conversions: 
    * Grid longitude is: 
        * coordinate longitude if it is positive
        * coordinate longitude + 720 if it is negative
    * Grid latitude is 180 - coordinate latitude

```{r}
lon_conversion = function(coord_lon){
  coord_lon_double = coord_lon * 2
  if(coord_lon_double < 0){
    grid_lon = 720 + coord_lon_double
  } else {
    grid_lon = coord_lon_double
  }
  return(grid_lon)
}
```
```{r}
lat_conversion = function(coord_lat){
  coord_lat_double = coord_lat * 2
  grid_lat = 180 - coord_lat_double
  return(grid_lat)
}
```
Write functions to convert coordinates to grid version of coordinates. 
```{r}
site_locations = site_locations %>% 
  rowwise() %>% 
  mutate(grid_lon = lon_conversion(lon), 
         grid_lat = lat_conversion(lat))
site_locations
```
```{r}
site_locations$grid_lon[2] = site_locations$grid_lon[2] + 2
site_locations
```
Convert all site coordinates to grid coordinates. 
```{r}
ggplot(single_year_temp, aes(lon, lat)) +
  geom_tile(aes(fill = temp)) +
  geom_point(data = site_locations, aes(x = grid_lon, y = grid_lat)) +
  scale_y_reverse()
```
Plot grid coordinates on one year of temperature to check accuracy. 
```{r}
months = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
years = c()
for(year in 1900:2014){
  year_repeats = rep(year, 12)
  years = c(years, year_repeats)
}
indices = data.frame(index = seq(1, 1380), month = rep(months, 115), year = years)
indices$month = factor(indices$month, levels = month.abb)
indices
```
Set up dataframe with month and year for each index value. 
```{r}
monthly_temps = data.frame(site = character(), month = numeric(), temp = numeric())
for(site in site_locations$site_name){
  lon = site_locations$grid_lon[site_locations$site_name == site]
  lat = site_locations$grid_lat[site_locations$site_name == site]
  for(index in indices$index){
    temp = temp_variable_all[lon, lat, index]
    each_temp = data.frame(site = site, index = index, temp = temp)
    monthly_temps = rbind(monthly_temps, each_temp)
  }
}
monthly_temps
```
Extract all monthly temperatures for each site. 
```{r}
monthly_temps = monthly_temps %>% 
  left_join(indices, by = "index")
monthly_temps
```
Add date information to monthly temperatures. 
```{r}
annual_temps = monthly_temps %>%
  group_by(site, year) %>% 
  summarise(avg_temp = mean(temp), 
            avg_temp_sd = sd(temp))
annual_temps
```
Get mean annual temperatures for all sites and put into dataframe. 
```{r}
ggplot(annual_temps, aes(x = year, y = avg_temp)) +
  geom_point() +
  geom_errorbar(aes(x = year, ymin = avg_temp - avg_temp_sd, ymax = avg_temp + avg_temp_sd)) +
  facet_wrap(~site)
```
Plot mean annual temperature and standard deviation for each site. 
